{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from qtorch.quant import *\n",
    "from qtorch.auto_low import *\n",
    "\n",
    "class SimpleLinearLP(nn.Module):\n",
    "    def __init__(self, quant, size=3072):\n",
    "        super(SimpleLinearLP, self).__init__()\n",
    "        self.classifier = nn.Linear(size, size)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "        self.quant = quant\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, x.size(0))\n",
    "        x = self.classifier(x)\n",
    "        x = self.quant(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleLinearManualAutoLP(nn.Module):\n",
    "    def __init__(self, quant, size=3072):\n",
    "        super(SimpleLinearManualAutoLP, self).__init__()\n",
    "        self.classifier = nn.Linear(size, size)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "        self.quant = quant\n",
    "        old_forward = self.classifier.forward\n",
    "        self.classifier.forward = lambda *inputs : self.quant(old_forward(*inputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, x.size(0))\n",
    "        x = self.classifier(x)\n",
    "#         x = self.quant(x)\n",
    "        return x\n",
    "\n",
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self, size=3072):\n",
    "        super(SimpleLinear, self).__init__()\n",
    "        self.classifier = nn.Linear(size, size)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, x.size(0))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class SimpleMatrix(nn.Module):\n",
    "    def __init__(self, size=3072):\n",
    "        super(SimpleMatrix, self).__init__()\n",
    "        self.classifier = nn.Linear(size, size)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SimpleMatrixLP(nn.Module):\n",
    "    def __init__(self, quant, size=3072):\n",
    "        super(SimpleMatrixLP, self).__init__()\n",
    "        self.classifier = nn.Linear(size, size)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "        self.quant = quant\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        x = self.quant(x)\n",
    "        return x\n",
    "\n",
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self, num_channels=10, kernel_size=3):\n",
    "        super(SimpleConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, num_channels, kernel_size, padding=1)\n",
    "        self.conv1.weight.data = torch.ones_like(self.conv1.weight.data)\n",
    "        self.conv1.bias.data = torch.ones_like(self.conv1.bias.data)\n",
    "\n",
    "        self.classifier = nn.Conv2d(num_channels, num_channels, kernel_size, padding=1)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SimpleConvLP(nn.Module):\n",
    "    def __init__(self, quant, num_channels=10, kernel_size=3):\n",
    "        super(SimpleConvLP, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, num_channels, kernel_size, padding=1)\n",
    "        self.conv1.weight.data = torch.ones_like(self.conv1.weight.data)\n",
    "        self.conv1.bias.data = torch.ones_like(self.conv1.bias.data)\n",
    "        self.quant = quant\n",
    "        self.classifier = nn.Conv2d(num_channels, num_channels, kernel_size, padding=1)\n",
    "        self.classifier.weight.data = torch.ones_like(self.classifier.weight.data)\n",
    "        self.classifier.bias.data = torch.ones_like(self.classifier.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.quant(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.quant(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress = True # Suppress printing statements\n",
    "\n",
    "import contextlib\n",
    "import sys\n",
    "\n",
    "# class DummyFile(object):\n",
    "#     def write(self, x): pass\n",
    "#     def flush(self): pass\n",
    "import io\n",
    "\n",
    "import functools\n",
    "def suppress_print(func):\n",
    "    if not suppress:\n",
    "        return func\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_decorator(*args, **kwargs):\n",
    "        save_stdout = sys.stdout\n",
    "        if suppress:\n",
    "            sys.stdout = io.StringIO()\n",
    "        value = func(*args, **kwargs)\n",
    "        sys.stdout = save_stdout\n",
    "        return value\n",
    "    return wrapper_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_timing()\n",
    "quant = Quantizer(16, 16, 16, 16, -1, -1, -1, -1, \"nearest\", \"nearest\", \"fixed\", \"fixed\")\n",
    "device=\"cpu\"\n",
    "model_1 = SimpleLinearLP(quant, size=10).to(device)\n",
    "a = torch.linspace(- 2 ** (16-16-1), 2 ** (16-16-1) - 2 ** (-16), steps=10, device=device)\n",
    "output_1 = model_1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@suppress_print\n",
    "def test():\n",
    "    print(\"nmb\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@suppress_print\n",
    "def conv_timing(conv_channel=512, input_size=3072, wl=16, fl=16, man=8, exp=7, rounding_type=\"nearest\", number_type=\"fixed\", device=\"cuda\"):\n",
    "    \n",
    "    if number_type in [\"fixed\", \"block\"]:\n",
    "        quant = Quantizer(wl, wl, fl, fl, -1, -1, -1, -1, rounding_type, rounding_type, number_type, number_type)\n",
    "    else:\n",
    "        quant = Quantizer(-1, -1, -1, -1, man, man, exp, exp, rounding_type, rounding_type, number_type, number_type)\n",
    "    \n",
    "    import math\n",
    "    width = int(math.sqrt(input_size / 3))\n",
    "    a = torch.linspace(- 2 ** (wl-fl-1), 2 ** (wl-fl-1) - 2 ** (-fl), steps=input_size, device=device)\n",
    "    a = a.reshape(1,3, width,width)\n",
    "    a_copy = a.clone()\n",
    "    print(f\"Test convolutional layer with {conv_channel} channels\")\n",
    "    model_1 = SimpleConvLP(quant, num_channels=conv_channel).to(device)\n",
    "    model_2 = SimpleConv(num_channels=conv_channel).to(device)\n",
    "    model_3 = SimpleConv(num_channels=conv_channel).to(device)\n",
    "    lower(model_2, \n",
    "          layer_types=[\"conv\"],\n",
    "          wl_activate=wl, \n",
    "          wl_error=wl,\n",
    "          fl_activate=fl, \n",
    "          fl_error=fl,\n",
    "          activate_rounding=rounding_type,\n",
    "          error_rounding=rounding_type,\n",
    "          activate_type=number_type,\n",
    "          error_type=number_type\n",
    "    )\n",
    "    \n",
    "    import time\n",
    "    output_1 = model_1(a)\n",
    "    output_1 = model_1(a)\n",
    "    output_1 = model_1(a)\n",
    "    loop = 1000\n",
    "    total_time = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_1 = model_1(a)\n",
    "        time_since_1 = time.time() - time_now\n",
    "        total_time += time_since_1\n",
    "\n",
    "    output_2 = model_2(a_copy)\n",
    "    output_2 = model_2(a_copy)\n",
    "    output_2 = model_2(a_copy)\n",
    "    total_time_2 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_2 = model_2(a_copy)\n",
    "        time_since_2 = time.time() - time_now\n",
    "        total_time_2 += time_since_2\n",
    "\n",
    "    output_3 = model_3(a_copy)\n",
    "    output_3 = model_3(a_copy)\n",
    "    output_3 = model_3(a_copy)\n",
    "    total_time_3 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_3 = model_3(a_copy)\n",
    "        time_since_3 = time.time() - time_now\n",
    "        total_time_3 += time_since_3\n",
    "\n",
    "    # print(torch.nn.L1Loss()(a,a_copy))\n",
    "    # print(torch.nn.L1Loss()(output_1,output_2))\n",
    "    # print(torch.nn.L1Loss()(output_1,output_3))\n",
    "    print(f\"Running time - Model with manually inserted LP layers: {total_time}\")\n",
    "    print(f\"Running time - Model with auto inserted LP layers: {total_time_2}\")\n",
    "    print(f\"Running time - Model with no LP layers: {total_time_3}\")\n",
    "    return total_time, total_time_2, total_time_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@suppress_print\n",
    "def linear_timing(input_size=10, wl=16, fl=16, rounding_type=\"nearest\", number_type=\"fixed\", device=\"cuda\"):\n",
    "    import pdb; pdb.set_trace();\n",
    "    if number_type in [\"fixed\", \"block\"]:\n",
    "        quant = Quantizer(wl, wl, fl, fl, -1, -1, -1, -1, rounding_type, rounding_type, number_type, number_type)\n",
    "    else:\n",
    "        quant = Quantizer(-1, -1, -1, -1, man, man, exp, exp, rounding_type, rounding_type, number_type, number_type)\n",
    "    \n",
    "    matrix_size = input_size\n",
    "    a = torch.linspace(- 2 ** (wl-fl-1), 2 ** (wl-fl-1) - 2 ** (-fl), steps=matrix_size, device=device)\n",
    "    a_copy = a.clone()\n",
    "    print(f\"Test linear layer with size {matrix_size} x {matrix_size}\")\n",
    "    model_1 = SimpleLinearLP(quant, size=matrix_size).to(device)\n",
    "    model_2 = SimpleLinear(size=matrix_size).to(device)\n",
    "    model_3 = SimpleLinear(size=matrix_size).to(device)\n",
    "    model_4 = SimpleLinearManualAutoLP(quant, size=matrix_size).to(device)\n",
    "    lower(model_2, \n",
    "          layer_types=[\"linear\"],\n",
    "          wl_activate=wl, \n",
    "          wl_error=wl,\n",
    "          fl_activate=fl, \n",
    "          fl_error=fl,\n",
    "          activate_rounding=rounding_type,\n",
    "          error_rounding=rounding_type,\n",
    "          activate_type=number_type,\n",
    "          error_type=number_type\n",
    "    )\n",
    "    import time\n",
    "    output_1 = model_1(a)\n",
    "    output_1 = model_1(a)\n",
    "    output_1 = model_1(a)\n",
    "    loop = 1000\n",
    "    total_time = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_1 = model_1(a)\n",
    "        time_since_1 = time.time() - time_now\n",
    "        total_time += time_since_1\n",
    "\n",
    "    output_2 = model_2(a_copy)\n",
    "    output_2 = model_2(a_copy)\n",
    "    output_2 = model_2(a_copy)\n",
    "    total_time_2 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_2 = model_2(a_copy)\n",
    "        time_since_2 = time.time() - time_now\n",
    "        total_time_2 += time_since_2\n",
    "\n",
    "    output_3 = model_3(a_copy)\n",
    "    output_3 = model_3(a_copy)\n",
    "    output_3 = model_3(a_copy)\n",
    "    total_time_3 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_3 = model_3(a_copy)\n",
    "        time_since_3 = time.time() - time_now\n",
    "        total_time_3 += time_since_3\n",
    "    \n",
    "    output_4 = model_4(a_copy)\n",
    "    output_4 = model_4(a_copy)\n",
    "    output_4 = model_4(a_copy)\n",
    "    total_time_4 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_4 = model_4(a_copy)\n",
    "        time_since_4 = time.time() - time_now\n",
    "        total_time_4 += time_since_4\n",
    "\n",
    "    # print(torch.nn.L1Loss()(a,a_copy))\n",
    "    # print(torch.nn.L1Loss()(output_1,output_2))\n",
    "    # print(torch.nn.L1Loss()(output_1,output_3))\n",
    "    print(f\"Running time - Model with manually inserted LP layers: {total_time}\")\n",
    "    print(f\"Running time - Model with auto inserted LP layers: {total_time_2}\")\n",
    "    print(f\"Running time - Model with no LP layers: {total_time_3}\")\n",
    "    print(f\"Running time - Model with manually inserted LP (lambda) layers: {total_time_4}\")\n",
    "    return total_time, total_time_2, total_time_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@suppress_print\n",
    "def matrix_timing(input_size=3072, wl=16, fl=16, rounding_type=\"nearest\", number_type=\"fixed\", device=\"cuda\"):\n",
    "\n",
    "    if number_type in [\"fixed\", \"block\"]:\n",
    "        quant = Quantizer(wl, wl, fl, fl, -1, -1, -1, -1, rounding_type, rounding_type, number_type, number_type)\n",
    "    else:\n",
    "        quant = Quantizer(-1, -1, -1, -1, man, man, exp, exp, rounding_type, rounding_type, number_type, number_type)\n",
    "    \n",
    "    matrix_size = input_size\n",
    "    a = torch.linspace(- 2 ** (wl-fl-1), 2 ** (wl-fl-1) - 2 ** (-fl), steps=matrix_size, device=device)\n",
    "    a = a.repeat(a.size(0))\n",
    "    a = a.reshape(matrix_size, matrix_size)\n",
    "    a_copy = a.clone()\n",
    "    print(f\"Test matrix x matrix with size {matrix_size} x {matrix_size}\")\n",
    "    model_1 = SimpleMatrixLP(quant, size=matrix_size).to(device)\n",
    "    model_2 = SimpleMatrix(size=matrix_size).to(device)\n",
    "    model_3 = SimpleMatrix(size=matrix_size).to(device)\n",
    "    lower(model_2, \n",
    "          layer_types=[\"linear\"],\n",
    "          wl_activate=wl, \n",
    "          wl_error=wl,\n",
    "          fl_activate=fl, \n",
    "          fl_error=fl,\n",
    "          activate_rounding=rounding_type,\n",
    "          error_rounding=rounding_type,\n",
    "          activate_type=number_type,\n",
    "          error_type=number_type\n",
    "    )\n",
    "    import time\n",
    "    output_1 = model_1(a)\n",
    "    output_1 = model_1(a)\n",
    "    output_1 = model_1(a)\n",
    "    loop = 1000\n",
    "    total_time = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_1 = model_1(a)\n",
    "        time_since_1 = time.time() - time_now\n",
    "        total_time += time_since_1\n",
    "\n",
    "    output_2 = model_2(a_copy)\n",
    "    output_2 = model_2(a_copy)\n",
    "    output_2 = model_2(a_copy)\n",
    "    total_time_2 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_2 = model_2(a_copy)\n",
    "        time_since_2 = time.time() - time_now\n",
    "        total_time_2 += time_since_2\n",
    "\n",
    "    output_3 = model_3(a_copy)\n",
    "    output_3 = model_3(a_copy)\n",
    "    output_3 = model_3(a_copy)\n",
    "    total_time_3 = 0\n",
    "    for i in range(loop):\n",
    "        time_now = time.time()\n",
    "        output_3 = model_3(a_copy)\n",
    "        time_since_3 = time.time() - time_now\n",
    "        total_time_3 += time_since_3\n",
    "\n",
    "    # print(torch.nn.L1Loss()(a,a_copy))\n",
    "    # print(torch.nn.L1Loss()(output_1,output_2))\n",
    "    # print(torch.nn.L1Loss()(output_1,output_3))\n",
    "    print(f\"Running time - Model with manually inserted LP layers: {total_time}\")\n",
    "    print(f\"Running time - Model with auto inserted LP layers: {total_time_2}\")\n",
    "    print(f\"Running time - Model with no LP layers: {total_time_3}\")\n",
    "    return total_time, total_time_2, total_time_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeing_plot(func, input_sizes, wl=16, fl=16, rounding_type=\"nearest\", number_type=\"fixed\", device=\"cpu\", title=\"Running Time of Linear layer\", xlabel=\"Input Size\"):\n",
    "\n",
    "    time_1_arr = []\n",
    "    time_2_arr = []\n",
    "    time_3_arr = []\n",
    "    for input_size in input_sizes:\n",
    "        time_1, time_2, time_3 = func(input_size, wl=wl, fl=fl, rounding_type=rounding_type, number_type=number_type, device=device)\n",
    "        time_1_arr.append(time_1)\n",
    "        time_2_arr.append(time_2)\n",
    "        time_3_arr.append(time_3)\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "#     input_sizes = [500, 1000, 1500, 2000, 2500, 3000, 5000, 8000]\n",
    "    # evenly sampled time at 200ms intervals\n",
    "    time_1_np = np.array(time_1_arr)\n",
    "    time_2_np = np.array(time_2_arr)\n",
    "    time_3_np = np.array(time_3_arr)\n",
    "    # red dashes, blue squares and green triangles\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Running Time')\n",
    "    plt.title(title)\n",
    "    plt.plot(input_sizes, time_1_np, 'r--', label= \"Manually Inserted LP\")\n",
    "    plt.plot(input_sizes, time_2_np, 'bs--', label=\"Auto Inserted LP\")\n",
    "    plt.plot(input_sizes, time_3_np, 'g--',  label=\"No LP\")\n",
    "    plt.legend(loc='upper left',bbox_to_anchor=(1,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = [500, 1000, 1500, 2000, 2500, 3000, 3500]\n",
    "input_sizes = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Linear layer (cpu + nearest_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Linear layer (cpu + stochastic_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Linear layer (cpu + nearest_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Linear layer (cpu + stochastic_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Linear layer (cpu + nearest_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Linear layer (cpu + stochastic_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Linear layer (cuda + nearest_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Linear layer (cuda + stochastic_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Linear layer (cuda + nearest_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Linear layer (cuda + stochastic_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Linear layer (cuda + nearest_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(linear_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Linear layer (cuda + stochastic_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = [500, 1000, 1500, 2000, 2500, 3000, 3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Matrix x Matrix (cpu + nearest_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Matrix x Matrix (cpu + stochastic_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Matrix x Matrix (cpu + nearest_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Matrix x Matrix (cpu + stochastic_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Matrix x Matrix (cpu + nearest_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Matrix x Matrix (cpu + stochastic_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Matrix x Matrix (cuda + nearest_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Matrix x Matrix (cuda + stochastic_round + fixed point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Matrix x Matrix (cuda + nearest_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Matrix x Matrix (cuda + stochastic_round + block floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Matrix x Matrix (cuda + nearest_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(matrix_timing, \n",
    "             input_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Matrix x Matrix (cuda + stochastic_round + floating point)\", \n",
    "             xlabel=\"Input Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_sizes = [500, 1000, 1500, 2000, 2500, 3000, 3500]\n",
    "channel_sizes = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Convolutional layer  (cpu + nearest_round + fixed point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Convolutional layer (cpu + stochastic_round + fixed point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Convolutional layer (cpu + nearest_round + block floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Convolutional layer (cpu + stochastic_round + block floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Convolutional layer (cpu + nearest_round + floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cpu\", \n",
    "             title=\"Running Time of Convolutional layer (cpu + stochastic_round + floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Convolutional layer (cuda + nearest_round + fixed point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"fixed\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Convolutional layer (cuda + stochastic_round + fixed point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Convolutional layer (cuda + nearest_round + block floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"block\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Convolutional layer (cuda + stochastic_round + block floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"nearest\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Convolutional layer (cuda + nearest_round + floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeing_plot(conv_timing, \n",
    "             channel_sizes, \n",
    "             rounding_type=\"stochastic\", \n",
    "             number_type=\"float\", \n",
    "             device=\"cuda\", \n",
    "             title=\"Running Time of Convolutional layer (cuda + stochastic_round + floating point)\", \n",
    "             xlabel=\"Channel Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
